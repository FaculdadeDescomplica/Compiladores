{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Célula 1 - Setup Completo (Lexer + Parser)"
      ],
      "metadata": {
        "id": "jchNRzZxVV3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Any, Optional\n",
        "\n",
        "# ===== Definição dos Tokens =====\n",
        "TOKEN_SPEC = [\n",
        "    (\"NUMBER\",   r\"\\d+(\\.\\d+)?\"),  # Números (inteiros e decimais)\n",
        "    (\"IDENT\",    r\"[A-Za-z_]\\w*\"), # Identificadores (letras e sublinhados)\n",
        "    (\"PLUS\", r\"\\+\"),                # Operador de soma\n",
        "    (\"MINUS\", r\"-\"),                # Operador de subtração\n",
        "    (\"STAR\", r\"\\*\"),                # Operador de multiplicação\n",
        "    (\"SLASH\", r\"/\"),                # Operador de divisão\n",
        "    (\"CARET\", r\"\\^\"),               # Operador de potência\n",
        "    (\"LPAREN\", r\"\\(\"),              # Parêntese esquerdo\n",
        "    (\"RPAREN\", r\"\\)\"),              # Parêntese direito\n",
        "    (\"SKIP\", r\"[ \\t\\r\\n]+\"),        # Ignora espaços, tabs e quebras de linha\n",
        "    (\"MISMATCH\", r\".\")              # Para caracteres que não se encaixam\n",
        "]\n",
        "\n",
        "# Compilando a expressão regular para o lexer\n",
        "MASTER_RE = re.compile(\"|\".join(f\"(?P<{name}>{regex})\" for name, regex in TOKEN_SPEC))\n",
        "\n",
        "@dataclass\n",
        "class Token:\n",
        "    type: str\n",
        "    value: str\n",
        "    pos: int\n",
        "\n",
        "def tokenize(code: str) -> List[Token]:\n",
        "    tokens = []\n",
        "    for m in MASTER_RE.finditer(code):\n",
        "        kind = m.lastgroup\n",
        "        value = m.group()\n",
        "        pos = m.start()\n",
        "        if kind == \"SKIP\":\n",
        "            continue\n",
        "        elif kind == \"MISMATCH\":\n",
        "            raise SyntaxError(f\"Caractere inesperado '{value}' na posição {pos}\")\n",
        "        tokens.append(Token(kind, value, pos))\n",
        "    return tokens\n",
        "\n",
        "# ===== Definição dos Nós da AST (Árvore Sintática) =====\n",
        "@dataclass\n",
        "class Num:\n",
        "    value: float\n",
        "\n",
        "@dataclass\n",
        "class Var:\n",
        "    name: str\n",
        "\n",
        "@dataclass\n",
        "class Unary:\n",
        "    op: str\n",
        "    expr: Any\n",
        "\n",
        "@dataclass\n",
        "class Binary:\n",
        "    op: str\n",
        "    left: Any\n",
        "    right: Any\n",
        "\n",
        "def ast_str(node: Any, indent: int = 0) -> str:\n",
        "    pad = \"  \" * indent\n",
        "    if isinstance(node, Num):\n",
        "        return f\"{pad}Num({node.value})\"\n",
        "    elif isinstance(node, Var):\n",
        "        return f\"{pad}Var({node.name})\"\n",
        "    elif isinstance(node, Unary):\n",
        "        return f\"{pad}Unary({node.op})\\n{ast_str(node.expr, indent + 1)}\"\n",
        "    elif isinstance(node, Binary):\n",
        "        return (f\"{pad}Binary({node.op})\\n\" +\n",
        "                f\"{ast_str(node.left, indent + 1)}\\n\" +\n",
        "                f\"{ast_str(node.right, indent + 1)}\")\n",
        "    return f\"{pad}{node!r}\"\n",
        "\n",
        "# ===== Parser (Analisador Sintático) =====\n",
        "class ParseError(Exception): ...\n",
        "\n",
        "class Parser:\n",
        "    def __init__(self, tokens: List[Token]):\n",
        "        self.tokens = tokens\n",
        "        self.i = 0\n",
        "\n",
        "    def peek(self, *types) -> Optional[Token]:\n",
        "        if self.i < len(self.tokens) and self.tokens[self.i].type in types:\n",
        "            return self.tokens[self.i]\n",
        "        return None\n",
        "\n",
        "    def eat(self, *types) -> Token:\n",
        "        if self.peek(*types):\n",
        "            t = self.tokens[self.i]\n",
        "            self.i += 1\n",
        "            return t\n",
        "        raise ParseError(f\"Esperado um dos tipos {types}, mas obtido {self.tokens[self.i].type}.\")\n",
        "\n",
        "    def parse(self) -> Any:\n",
        "        node = self.expr()\n",
        "        if self.i != len(self.tokens):\n",
        "            raise ParseError(\"Entrada extra após a expressão.\")\n",
        "        return node\n",
        "\n",
        "    # Define regras para expressões, termos, potências e unidades\n",
        "    def expr(self) -> Any:\n",
        "        node = self.term()\n",
        "        while self.peek(\"PLUS\", \"MINUS\"):\n",
        "            op = self.eat(\"PLUS\", \"MINUS\").value\n",
        "            node = Binary(op, node, self.term())\n",
        "        return node\n",
        "\n",
        "    def term(self) -> Any:\n",
        "        node = self.power()\n",
        "        while self.peek(\"STAR\", \"SLASH\"):\n",
        "            op = self.eat(\"STAR\", \"SLASH\").value\n",
        "            node = Binary(op, node, self.power())\n",
        "        return node\n",
        "\n",
        "    def power(self) -> Any:\n",
        "        node = self.unary()\n",
        "        while self.peek(\"CARET\"):\n",
        "            self.eat(\"CARET\")\n",
        "            node = Binary(\"^\", node, self.unary())\n",
        "        return node\n",
        "\n",
        "    def unary(self) -> Any:\n",
        "        if self.peek(\"MINUS\"):\n",
        "            self.eat(\"MINUS\")\n",
        "            return Unary(\"-\", self.unary())\n",
        "        return self.primary()\n",
        "\n",
        "    def primary(self) -> Any:\n",
        "        if self.peek(\"NUMBER\"):\n",
        "            tok = self.eat(\"NUMBER\")\n",
        "            return Num(float(tok.value))\n",
        "        if self.peek(\"LPAREN\"):\n",
        "            self.eat(\"LPAREN\")\n",
        "            node = self.expr()\n",
        "            self.eat(\"RPAREN\")\n",
        "            return node\n",
        "        raise ParseError(\"Elemento primário inválido.\")\n",
        "\n",
        "# Teste inicial da Tokenização e AST\n",
        "test_code = \"3 + 4 * (2 - 1) ^ 2\"\n",
        "tokens = tokenize(test_code)\n",
        "print(\"Tokens:\", [(t.type, t.value) for t in tokens])\n",
        "\n",
        "# Criando o Parser e gerando a AST\n",
        "parser = Parser(tokens)\n",
        "ast = parser.parse()\n",
        "print(\"AST:\", ast_str(ast))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q40juG6RWgPU",
        "outputId": "6372cd87-88d2-4897-a8fd-9d859f076784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: [('NUMBER', '3'), ('PLUS', '+'), ('NUMBER', '4'), ('STAR', '*'), ('LPAREN', '('), ('NUMBER', '2'), ('MINUS', '-'), ('NUMBER', '1'), ('RPAREN', ')'), ('CARET', '^'), ('NUMBER', '2')]\n",
            "AST: Binary(+)\n",
            "  Num(3.0)\n",
            "  Binary(*)\n",
            "    Num(4.0)\n",
            "    Binary(^)\n",
            "      Binary(-)\n",
            "        Num(2.0)\n",
            "        Num(1.0)\n",
            "      Num(2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Célula 2 - Visualizando o Pipeline de Tokenização e Análise Sintática"
      ],
      "metadata": {
        "id": "BPTw6hBaVe0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(expr: str):\n",
        "    print(f\"\\n=== Processando a expressão: {expr}\")\n",
        "\n",
        "    # Tokenização\n",
        "    tokens = tokenize(expr)\n",
        "    print(\"Tokens:\", [(t.type, t.value) for t in tokens])\n",
        "\n",
        "    # Construção da AST\n",
        "    parser = Parser(tokens)\n",
        "    ast = parser.parse()\n",
        "    print(\"\\nÁrvore Sintática (AST):\")\n",
        "    print(ast_str(ast))\n",
        "\n",
        "    # Você pode adicionar aqui a fase de otimização ou outras análises, conforme necessário\n",
        "    print(\"\\nA análise léxica foi completada com sucesso!\")\n",
        "\n",
        "# Testando o pipeline com uma expressão matemática simples\n",
        "run_pipeline(\"3 + 4 * (2 - 1) ^ 2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y-VjFigWnCx",
        "outputId": "2eedfdb9-d4e7-44a7-8397-a674f375878a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Processando a expressão: 3 + 4 * (2 - 1) ^ 2\n",
            "Tokens: [('NUMBER', '3'), ('PLUS', '+'), ('NUMBER', '4'), ('STAR', '*'), ('LPAREN', '('), ('NUMBER', '2'), ('MINUS', '-'), ('NUMBER', '1'), ('RPAREN', ')'), ('CARET', '^'), ('NUMBER', '2')]\n",
            "\n",
            "Árvore Sintática (AST):\n",
            "Binary(+)\n",
            "  Num(3.0)\n",
            "  Binary(*)\n",
            "    Num(4.0)\n",
            "    Binary(^)\n",
            "      Binary(-)\n",
            "        Num(2.0)\n",
            "        Num(1.0)\n",
            "      Num(2.0)\n",
            "\n",
            "A análise léxica foi completada com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Célula 3 - Exercício e Análise Passo a Passo"
      ],
      "metadata": {
        "id": "-qDFI_uSVmP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    while True:\n",
        "        s = input(\"\\nDigite uma expressão (ENTER para sair): \").strip()\n",
        "        if not s: break\n",
        "        run_pipeline(s)\n",
        "except KeyboardInterrupt:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Hxfs6F5Wuws",
        "outputId": "7f800eaf-c791-40ea-c48d-276dde0010a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Digite uma expressão (ENTER para sair): -3 + 4 * 2\n",
            "\n",
            "=== Processando a expressão: -3 + 4 * 2\n",
            "Tokens: [('MINUS', '-'), ('NUMBER', '3'), ('PLUS', '+'), ('NUMBER', '4'), ('STAR', '*'), ('NUMBER', '2')]\n",
            "\n",
            "Árvore Sintática (AST):\n",
            "Binary(+)\n",
            "  Unary(-)\n",
            "    Num(3.0)\n",
            "  Binary(*)\n",
            "    Num(4.0)\n",
            "    Num(2.0)\n",
            "\n",
            "A análise léxica foi completada com sucesso!\n"
          ]
        }
      ]
    }
  ]
}